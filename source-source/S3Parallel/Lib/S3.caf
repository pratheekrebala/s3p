import &StandardImport, &FsEasy, &LibMisc, {} &shellEscape, &util.promisify, &fs, &path
escape = (single) -> shellEscape [] single

&AwsSdk.config.setPromisesDependency &bluebird

class S3 extends BaseClass

  @awsSdkS3 = new &AwsSdk.S3 {} # useAccelerateEndpoint: true
  # https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration-examples.html

  @classGetter
    s3: -> @awsSdkS3

  @listBuckets: =>
    @s3.listBuckets()
    .promise()
    .then ({Buckets}) ->
      object {Name, CreationDate} from Buckets with-key Name with CreationDate

  @list: ({bucket, prefix, limit=1000, requestPayer="requester", startAfter}) =>
    startTime = currentSecond()
    @s3.listObjectsV2
      Bucket:     bucket
      Prefix:     prefix
      MaxKeys:    limit
      RequestPayer: requestPayer
      StartAfter: startAfter
    .promise()
    .tapCatch (error) ->
      log.error S3.list-error: {} bucket, prefix, startAfter, limit, error

    .then (results) ->
      duration = currentSecond() - startTime
      unless results.Contents is Array
        log.warn S3.list-no-Contents: {} bucket, prefix, startAfter, limit, duration, results
        throw new Error "S3.list: Contents is not an array"

      else if duration > 60
        log.warn S3.list-slow: {} bucket, prefix, startAfter, limit, duration, results: merge results, Contents: "Array #{results.Contents.length}"

      results.Contents

  ##
    TODO:
      For large files (>5GB), we need to just use the CLI:
        aws s3 cp s3://resbio-fastq-drop-internal/180316_NB501104_0354_AHM2L2BGX5/Undetermined/Undetermined_S0_L001_R1_001.fastq.gz s3://genui-dynamodb-migration-scratch/large-test.gz
        We can add --quiet (probably)
      Further, that should use acceleration, if enabled - still need to test that.
    IN:
      options:
        size:       Bytes (optional) if provided and if bigger than the max size allowed by copyObject, largeCopy is used
        bucket:     <String> (ignored if both from-bucket and to-bucket are provided)
        fromBucket: from-bucket (default: bucket)
        toBucket:   to-bucket (default: bucket)
        key:      <String>
        fromKey:  from-key (default: key)
        toKey:    to-key (default: key)

        toFolder:   local folder
    OUT:
      Promise.then -> see https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#copyObject-property > Callback > Parameters
        {} CopyObjectResult, ...
  @copy: (options) =>
    if options extract scratchState
      copyScratchState = scratchState.copyScratchState ?= {}

    @_normalizeCopyOptions(options) extract
      fromBucket, toBucket, fromKey, toKey, size
      toFolder
      pretend
      verbose

    # NOTE: there appears to be a bug in the AWS s3 sdk copyObject - it can't handle spaces in the toKey
    if size >= 1024**3 || /\s/.test toKey
      # log largeCopy: {} fromBucket, toBucket, fromKey, toKey, size
      @largeCopy {} fromBucket, toBucket, toFolder, fromKey, toKey, pretend, verbose

    else
      copyOptions = if toFolder
        Bucket:     fromBucket
        Key:        fromKey
        RequestPayer: "requester"
        ACL:  "bucket-owner-full-control"
      else
        CopySource: "" #{fromBucket}/#{fromKey}
        Bucket:     toBucket
        Key:        toKey
        RequestPayer: "requester"
        ACL:  "bucket-owner-full-control"

      if verbose
        log.unquoted copyObject: copyOptions

      if pretend
        timeout 1 -> pretend: true
      else if toFolder
        createWriteStreamSafe
          path.join toFolder, fromKey
          copyScratchState

        .then (writeStream) -> new Promise (resolve, reject) ->
          @s3.getObject copyOptions
          .createReadStream()
          .pipe writeStream
          .on :finish resolve
          .on :error  reject

      else
        @s3.copyObject copyOptions
        .promise()

  @delete: (options) =>
    @s3.deleteObject
      Bucket: options.bucket
      Key:    options.key

  @largeCopy: (options) =>
    @_normalizeCopyOptions(options) extract fromBucket, fromFolder, toBucket, toFolder, fromKey, toKey, pretend, verbose

    command =
      ""
        aws s3 cp
        #{} escape createS3Url fromBucket, fromFolder, fromKey
        #{} escape createS3Url toBucket, toFolder, toKey
    if verbose
      log.unquoted command

    if pretend
      timeout 1 -> pretend: true
    else
      shellExec command

  # get object's metdata
  @headObject: ({bucket, key}) =>
    @s3.headObject
      Bucket: bucket
      Key: key
      RequestPayer: "requester"
    .promise()

  @_normalizeCopyOptions: (options) ->
    options extract
      bucket, key
      fromBucket = bucket
      toBucket = bucket
      fromKey = key
      toKey = key
      size

    if isFunction toKey
      toKey = toKey fromKey, fromBucket, toBucket, options.size

    unless present(fromBucket) && present(toBucket) && present(fromKey) && present toKey
      throw new Error "" Missing one of: fromBucket, toBucket, fromKey, toKey or bucket or key as a default
    merge options, {} fromBucket, fromKey, toBucket, toKey

  @shouldSyncObjects: (options) ->
    @_normalizeCopyOptions(options) extract fromBucket, toBucket, fromKey, toKey
    Promise.then ->
      if options.size < 1024**2 || !options.size
        true
      else
        @headObject options
        .then ({ContentLength}) -> ContentLength != options.size

  ## syncObject
    IN: options:
      passed to copyObject and shouldSyncObject
      size: used to compare and decide if it should copy
    OUT:
      copied: Bool
      status: if not copied, explains why
  @syncObject: (options) =>
    @shouldSyncObjects options
    .then (shouldSync) ->
      if shouldSync
        @copyObject options
        .then (result) -> merge result, copied: true

      else {} copied: false
        # status: "s3://#{toBucket}/#{toKey} exists and has same size (#{size}); skipping"
